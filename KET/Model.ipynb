{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation = Enum('Activation', 'ReLU Swish GeLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import TransfoXLTokenizer, TransfoXLModel, TransfoXLLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.activation import ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from fastai.text.models import MultiHeadRelativeAttention,DecoderLayer,MultiHeadAttention,feed_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHeadRelativeAttention??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiHeadAttention??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransfoXLLMHeadModel(\n",
       "  (transformer): TransfoXLModel(\n",
       "    (word_emb): AdaptiveEmbedding(\n",
       "      (emb_layers): ModuleList(\n",
       "        (0): Embedding(20000, 1024)\n",
       "        (1): Embedding(20000, 256)\n",
       "        (2): Embedding(160000, 64)\n",
       "        (3): Embedding(67735, 16)\n",
       "      )\n",
       "      (emb_projs): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (4): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (5): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (6): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (7): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (8): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (9): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (10): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (11): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (12): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (13): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (14): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (15): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (16): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (17): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "  )\n",
       "  (crit): ProjectedAdaptiveLogSoftmax(\n",
       "    (out_layers): ModuleList(\n",
       "      (0): Linear(in_features=1024, out_features=20000, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=20000, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=160000, bias=True)\n",
       "      (3): Linear(in_features=16, out_features=67735, bias=True)\n",
       "    )\n",
       "    (out_projs): ParameterList(\n",
       "        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "        (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "        (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "        (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary from wikitext 103)\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
    "\n",
    "# Tokenized input\n",
    "text_1 = \"Who was Jim Henson ?\"\n",
    "text_2 = \"Jim Henson was a puppeteer\"\n",
    "tokenized_text_1 = tokenizer.tokenize(text_1)\n",
    "tokenized_text_2 = tokenizer.tokenize(text_2)\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens_1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
    "indexed_tokens_2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor_1 = tokens_tensor_1.to('cuda')\n",
    "tokens_tensor_2 = tokens_tensor_2.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Predict all tokens\n",
    "    predictions_1, mems_1 = model(tokens_tensor_1)\n",
    "    # We can re-use the memory cells in a subsequent call to attend a longer context\n",
    "    predictions_2, mems_2 = model(tokens_tensor_2, mems=mems_1)\n",
    "\n",
    "# get the predicted last token\n",
    "predicted_index = list(map(lambda x:x.item(),predictions_2[0, :, :].argmax(1)))\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    hidden_2, mems_2 = model.transformer(tokens_tensor_2, mems=mems_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1199,  0.1363,  0.1029,  ..., -0.3195, -0.3179, -0.0682],\n",
       "         [ 0.2988,  0.0126, -0.2789,  ..., -0.3445, -0.0766,  0.0204],\n",
       "         [ 0.0339,  0.0836, -0.2165,  ..., -0.3910, -0.1606,  0.0165],\n",
       "         [ 0.1092,  0.2094, -0.1813,  ..., -0.4091, -0.1013, -0.0221],\n",
       "         [-0.1104,  0.0851, -0.0909,  ..., -0.0560, -0.0288,  0.0116]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "267734"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransfoXLModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.children of TransfoXLLMHeadModel(\n",
       "  (transformer): TransfoXLModel(\n",
       "    (word_emb): AdaptiveEmbedding(\n",
       "      (emb_layers): ModuleList(\n",
       "        (0): Embedding(20000, 1024)\n",
       "        (1): Embedding(20000, 256)\n",
       "        (2): Embedding(160000, 64)\n",
       "        (3): Embedding(67735, 16)\n",
       "      )\n",
       "      (emb_projs): ParameterList(\n",
       "          (0): Parameter containing: [torch.cuda.FloatTensor of size 1024x1024 (GPU 0)]\n",
       "          (1): Parameter containing: [torch.cuda.FloatTensor of size 1024x256 (GPU 0)]\n",
       "          (2): Parameter containing: [torch.cuda.FloatTensor of size 1024x64 (GPU 0)]\n",
       "          (3): Parameter containing: [torch.cuda.FloatTensor of size 1024x16 (GPU 0)]\n",
       "      )\n",
       "    )\n",
       "    (drop): Dropout(p=0.1)\n",
       "    (layers): ModuleList(\n",
       "      (0): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (1): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (2): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (3): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (4): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (5): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (6): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (7): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (8): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (9): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (10): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (11): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (12): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (13): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (14): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (15): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (16): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "      (17): RelPartialLearnableDecoderLayer(\n",
       "        (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "          (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (drop): Dropout(p=0.1)\n",
       "          (dropatt): Dropout(p=0.0)\n",
       "          (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (layer_norm): BertLayerNorm()\n",
       "          (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (pos_ff): PositionwiseFF(\n",
       "          (CoreNet): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): ReLU(inplace)\n",
       "            (2): Dropout(p=0.1)\n",
       "            (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (4): Dropout(p=0.1)\n",
       "          )\n",
       "          (layer_norm): BertLayerNorm()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pos_emb): PositionalEmbedding()\n",
       "  )\n",
       "  (crit): ProjectedAdaptiveLogSoftmax(\n",
       "    (out_layers): ModuleList(\n",
       "      (0): Linear(in_features=1024, out_features=20000, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=20000, bias=True)\n",
       "      (2): Linear(in_features=64, out_features=160000, bias=True)\n",
       "      (3): Linear(in_features=16, out_features=67735, bias=True)\n",
       "    )\n",
       "    (out_projs): ParameterList(\n",
       "        (0): Parameter containing: [torch.cuda.FloatTensor of size 1024x1024 (GPU 0)]\n",
       "        (1): Parameter containing: [torch.cuda.FloatTensor of size 1024x256 (GPU 0)]\n",
       "        (2): Parameter containing: [torch.cuda.FloatTensor of size 1024x64 (GPU 0)]\n",
       "        (3): Parameter containing: [torch.cuda.FloatTensor of size 1024x16 (GPU 0)]\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_wrd_conf = {\n",
    "    \"d_model\":1024,\n",
    "    \"embed_p\":0.,\n",
    "    \"n_heads\":5,\n",
    "    \"d_head\":64,\n",
    "    \"resid_p\":0.0,\n",
    "    \"attn_p\":0.0,\n",
    "    \"bias\": False,\n",
    "    \"scale\":True,\n",
    "    \"intermediate_size\":3072,\n",
    "    \"ff_p\":0.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ent_conf = default_wrd_conf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ent_conf[\"d_model\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhra_keys = [\"d_model\",\"n_heads\",\"d_head\",\"resid_p\",\"attn_p\",\"bias\",\"scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 100,\n",
       " 'n_heads': 5,\n",
       " 'd_head': 64,\n",
       " 'resid_p': 0.0,\n",
       " 'attn_p': 0.0,\n",
       " 'bias': False,\n",
       " 'scale': True}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ k: default_ent_conf[k] for k in [\"d_model\",\"n_heads\",\"d_head\",\"resid_p\",\"attn_p\",\"bias\",\"scale\"] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntEmbedingLayer(torch.nn.Module):\n",
    "    def __init__(vocab_sz_ent:int,ctx_len:int,d_model_ent:int):\n",
    "        super().__init__()\n",
    "        self.ent_enc = nn.Embedding(vocab_sz_ent, d_model_ent)\n",
    "\n",
    "    def forword(ent_ids_tensor):\n",
    "        return self.ent_enc(ent_ids_tensor)\n",
    "\n",
    "class KETFeedForward(torch.nn.Module):\n",
    "    def __init__(self, wrd_conf,ent_conf):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(wrd_conf[\"intermediate_size\"], wrd_conf[\"d_model\"])\n",
    "        self.dense_ent = nn.Linear(wrd_conf[\"intermediate_size\"], ent_conf[\"d_model\"])\n",
    "        self.LayerNorm = nn.LayerNorm(wrd_conf[\"d_model\"], eps=1e-12)\n",
    "        self.LayerNorm_ent = nn.LayerNorm(ent_conf[\"d_model\"], eps=1e-12)\n",
    "        self.dropout = nn.Dropout(wrd_conf[\"ff_p\"])\n",
    "\n",
    "    def forward(self, hidden_states_, input_tensor, input_tensor_ent):\n",
    "        hidden_states = self.dense(hidden_states_)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        hidden_states_ent = self.dense_ent(hidden_states_)\n",
    "        hidden_states_ent = self.dropout(hidden_states_ent)\n",
    "        hidden_states_ent = self.LayerNorm_ent(hidden_states_ent + input_tensor_ent)\n",
    "        return hidden_states, hidden_states_ent\n",
    "\n",
    "class MaskedKnowledgeAGGBlock(torch.nn.Module):\n",
    "    def __init__(self,wrd_conf,ent_conf):\n",
    "        super().__init__()\n",
    "        self.mhra_wrd = MultiHeadRelativeAttention(**{ k: wrd_conf[k] for k in mhra_keys })\n",
    "        self.mhra_ent = MultiHeadRelativeAttention(**{ k: ent_conf[k] for k in mhra_keys })\n",
    "        self.dense = nn.Linear(wrd_conf[\"d_model\"], wrd_conf[\"intermediate_size\"])\n",
    "        self.dense_ent = nn.Linear(ent_conf[\"d_model\"], wrd_conf[\"intermediate_size\"])\n",
    "        self.act = ReLU()\n",
    "        self.ff = KETFeedForward(wrd_conf,ent_conf)\n",
    "\n",
    "    def forward(self,hidden,hidden_ent,mask,mask_ent,r, u, v,r_ent,u_ent,v_ent, mem,mem_ent):\n",
    "        hidden = self.mhra_wrd(hidden,r=r, u=u, v=v,mask=mask,mem=mem)\n",
    "        hidden_ent = self.mhra_ent(hidden_ent,r=r_ent, u=u_ent, v=v_ent,mask=mask_ent,mem=mem_ent)\n",
    "        # get knowledge and words to same dim\n",
    "        intermediate = self.dense(hidden)+self.dense_ent(hidden_ent)\n",
    "        intermediate = self.act(intermediate_size)\n",
    "        # mixes the knowledge with the tokens\n",
    "        hidden,hidden_ent = self.ff(intermediate,hidden,hidden_ent)\n",
    "    \n",
    "\n",
    "def create_MKAGG_submodules(ctx_len,conf):\n",
    "    pos_enc = nn.Embedding(ctx_len, conf[\"d_model\"])\n",
    "    drop_emb = nn.Dropout(conf[\"embed_p\"])\n",
    "    u = nn.Parameter(torch.Tensor(conf[\"n_heads\"], 1, conf[\"d_head\"]))\n",
    "    v = nn.Parameter(torch.Tensor(conf[\"n_heads\"], 1, conf[\"d_head\"]))\n",
    "    return pos_enc,drop_emb,u,v\n",
    "\n",
    "class MaskedKnowledgeAGG(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 wrd_conf,\n",
    "                    #  d_model:int,\n",
    "                    #  d_head:int,\n",
    "                    #  n_heads:int,\n",
    "                    #  d_inner: int,\n",
    "                    #  embed_p: float=0.,\n",
    "                    #  resid_p: float=0.,\n",
    "                    #  attn_p: float=0.,\n",
    "                    #  ff_p: float=0.,\n",
    "  \n",
    "                 ent_conf,\n",
    "                    #  d_model_ent: int,\n",
    "                    #  d_head_ent: int,\n",
    "                    #  n_heads_ent: int,\n",
    "                    #  d_inner_ent: int,\n",
    "                    #  embed_p_ent: float=0.,\n",
    "                    #  resid_p_ent: float=0.,\n",
    "                    #  attn_p_ent: float=0.,\n",
    "                    #  ff_p_ent: float=0.,\n",
    "                 \n",
    "                 ctx_len: int,\n",
    "                 mem_len:int,\n",
    "                 n_layers:int,\n",
    "\n",
    "                 mask:bool=True,\n",
    "\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # embeding should be done in before this module layers\n",
    "        # self.token_enc = nn.Embedding(vocab_sz, d_model)\n",
    "        self.pos_enc_wrd, self.drop_emb_wrd, self.u_wrd, self.v_wrd = create_MKAGG_submodules(ctx_len,wrd_conf)\n",
    "        self.pos_enc_ent, self.drop_emb_ent, self.u_ent, self.v_ent = create_MKAGG_submodules(ctx_len,ent_conf)\n",
    "        self.mem_len,self.n_layers,self.mask = mem_len,n_layers,mask\n",
    "        \n",
    "        self.layers = []\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(MaskedKnowledgeAGGBlock(wrd_conf,ent_conf))\n",
    "            \n",
    "        self.layers =  nn.ModuleList(self.layers)\n",
    "\n",
    "\n",
    "\n",
    "    def _update_mems(self, mem,hids):\n",
    "        if mem is None:\n",
    "             return None\n",
    "        assert len(hids) == len(mem), 'len(hids) != len(self.hidden)'\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(hids)):\n",
    "                cat = torch.cat([mem[1], hids[i]], dim=1)\n",
    "            return  cat[:,-self.mem_len:].detach()\n",
    "\n",
    "    def forward(self, x,x_ent,mem,mem_ent):\n",
    "        #The hidden state has to be initiliazed in the forward pass for nn.DataParallel\n",
    "        bs,x_len = x.size()\n",
    "        inp = self.drop_emb_wrd(x) #.mul_(self.d_model ** 0.5)\n",
    "        inp_ent = self.drop_emb_ent(x_ent)\n",
    "\n",
    "        m_len = mem[0].size(1) if len(self.mem[0].size()) > 1 else 0\n",
    "        \n",
    "        seq_len = m_len + x_len\n",
    "        \n",
    "        mask = torch.triu(x.new_ones(x_len, seq_len), diagonal=1+m_len).byte()[None,None] if self.mask else None\n",
    "         # need to addapt this map acording to traning task (\n",
    "         #  for next token prediction dont mask next entity\n",
    "         #  this will train the network to predict next token with signals from next entity\n",
    "         #  for next entity prediction mask next entity to evoid cheating\n",
    "         # )\n",
    "        mask_ent = torch.triu(x.new_ones(x_len, seq_len), diagonal=1+m_len).byte()[None,None] if self.mask_ent else None     \n",
    "\n",
    "        #[None,:,:None] for einsum implementation of attention\n",
    "        hids = []\n",
    "        hids_ent = []\n",
    "        pos = torch.arange(seq_len-1, -1, -1, device=inp.device, dtype=inp.dtype)\n",
    "\n",
    "        pos_enc = self.pos_enc_wrd(pos)\n",
    "        pos_enc_ent = self.pos_enc_ent(pos)\n",
    "\n",
    "        hids.append(inp)\n",
    "        hids_ent.append(inp_ent)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # mem = self.hidden[i] if self.mem_len > 0 else None\n",
    "\n",
    "            inp,inp_ent = layer(inp,inp_ent, r=pos_enc, u=self.u_wrd, v=self.v_wrd, mask=mask, mem=mem[i],r_net=pos_enc_ent, u_ent=self.u_ent,v_ent=self.v_ent,mask_ent=mask_ent,mem_ent=mem_ent[i])\n",
    "            hids.append(inp)\n",
    "            hids_ent.append(inp_ent)\n",
    "        # core_out = inp[:,-x_len:]\n",
    "\n",
    "        mem =  self._update_mems(mem,hids)\n",
    "        mem_ent = self._update_mems(mem_ent,hids_ent)\n",
    "        return mem,mem_ent ,inp,inp_ent\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedKnowledgeAGG(default_wrd_conf,default_ent_conf,512,512,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedKnowledgeAGG(\n",
       "  (pos_enc_wrd): Embedding(512, 1024)\n",
       "  (drop_emb_wrd): Dropout(p=0.0)\n",
       "  (pos_enc_ent): Embedding(512, 100)\n",
       "  (drop_emb_ent): Dropout(p=0.0)\n",
       "  (layers): ModuleList(\n",
       "    (0): MaskedKnowledgeAGGBlock(\n",
       "      (mhra_wrd): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=1024, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=1024, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      )\n",
       "      (mhra_ent): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=100, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=100, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=100, out_features=320, bias=False)\n",
       "      )\n",
       "      (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "      (dense_ent): Linear(in_features=100, out_features=3072, bias=True)\n",
       "      (act): ReLU()\n",
       "      (ff): KETFeedForward(\n",
       "        (dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (dense_ent): Linear(in_features=3072, out_features=100, bias=True)\n",
       "        (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (LayerNorm_ent): LayerNorm(torch.Size([100]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "    )\n",
       "    (1): MaskedKnowledgeAGGBlock(\n",
       "      (mhra_wrd): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=1024, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=1024, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      )\n",
       "      (mhra_ent): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=100, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=100, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=100, out_features=320, bias=False)\n",
       "      )\n",
       "      (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "      (dense_ent): Linear(in_features=100, out_features=3072, bias=True)\n",
       "      (act): ReLU()\n",
       "      (ff): KETFeedForward(\n",
       "        (dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (dense_ent): Linear(in_features=3072, out_features=100, bias=True)\n",
       "        (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (LayerNorm_ent): LayerNorm(torch.Size([100]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "    )\n",
       "    (2): MaskedKnowledgeAGGBlock(\n",
       "      (mhra_wrd): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=1024, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=1024, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      )\n",
       "      (mhra_ent): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=100, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=100, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=100, out_features=320, bias=False)\n",
       "      )\n",
       "      (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "      (dense_ent): Linear(in_features=100, out_features=3072, bias=True)\n",
       "      (act): ReLU()\n",
       "      (ff): KETFeedForward(\n",
       "        (dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (dense_ent): Linear(in_features=3072, out_features=100, bias=True)\n",
       "        (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (LayerNorm_ent): LayerNorm(torch.Size([100]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "    )\n",
       "    (3): MaskedKnowledgeAGGBlock(\n",
       "      (mhra_wrd): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=1024, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=1024, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      )\n",
       "      (mhra_ent): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=100, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=100, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=100, out_features=320, bias=False)\n",
       "      )\n",
       "      (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "      (dense_ent): Linear(in_features=100, out_features=3072, bias=True)\n",
       "      (act): ReLU()\n",
       "      (ff): KETFeedForward(\n",
       "        (dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (dense_ent): Linear(in_features=3072, out_features=100, bias=True)\n",
       "        (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (LayerNorm_ent): LayerNorm(torch.Size([100]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "    )\n",
       "    (4): MaskedKnowledgeAGGBlock(\n",
       "      (mhra_wrd): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=1024, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=1024, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=1024, out_features=320, bias=False)\n",
       "      )\n",
       "      (mhra_ent): MultiHeadRelativeAttention(\n",
       "        (attention): Linear(in_features=100, out_features=960, bias=False)\n",
       "        (out): Linear(in_features=320, out_features=100, bias=False)\n",
       "        (drop_att): Dropout(p=0.0)\n",
       "        (drop_res): Dropout(p=0.0)\n",
       "        (ln): LayerNorm(torch.Size([100]), eps=1e-05, elementwise_affine=True)\n",
       "        (r_attn): Linear(in_features=100, out_features=320, bias=False)\n",
       "      )\n",
       "      (dense): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "      (dense_ent): Linear(in_features=100, out_features=3072, bias=True)\n",
       "      (act): ReLU()\n",
       "      (ff): KETFeedForward(\n",
       "        (dense): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "        (dense_ent): Linear(in_features=3072, out_features=100, bias=True)\n",
       "        (LayerNorm): LayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (LayerNorm_ent): LayerNorm(torch.Size([100]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Environment (conda_fastai)",
   "language": "python",
   "name": "conda_fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
